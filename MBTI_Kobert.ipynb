{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZ8fH3aplMBL","executionInfo":{"status":"ok","timestamp":1717821059775,"user_tz":-540,"elapsed":15878,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"outputId":"78e8928d-59fd-4ed5-dbfa-b8f59f2df6d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/mbti_dataset.csv')\n","#print(df.columns)\n","#print(df.head())\n","# MBTI 레이블을 개별 차원으로 분할\n","def split_mbti_label(label):\n","    dimensions = ['e', 'i'], ['s', 'n'], ['t', 'f'], ['j', 'p']\n","    result = []\n","    for options in dimensions:\n","        if label[dimensions.index(options)] == options[0]:\n","            result.append(0)  # 첫 번째 옵션 (e, s, t, j)\n","        else:\n","            result.append(1)  # 두 번째 옵션 (i, n, f, p)\n","    return result\n","\n","# apply 함수를 사용하여 label을 분할하고 새로운 열을 생성\n","df[['label_EI', 'label_SN', 'label_TF', 'label_JP']] = df['label'].apply(split_mbti_label).apply(pd.Series)\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70DH9lw4dYKU","executionInfo":{"status":"ok","timestamp":1717642343187,"user_tz":-540,"elapsed":4998,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"outputId":"e2be11a9-e808-4e84-81ac-1ed5dcabdbf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                text label  label_EI  \\\n","0         공방 가서 맞추는 건 싫고 금으로 맞추는 거 아님[SEP]싫어요 ㅎ[SEP]  istj         1   \n","1                  원하시는 일상 공유가 어떤 건지 [SEP]궁금해요 [SEP]  intp         1   \n","2                  저는 무조건 ..![SEP]말해야 된다고 생각해요 [SEP]  enfp         0   \n","3  자기 정도만 쓰는 거 같아요.[SEP] 애인을 지칭할 때 '너'라고 하기에는 너무 ...  istj         1   \n","4  직장 상사 스타일이라고 생각하심 돼요 [SEP]제가 느끼기에는 f 부분이 너무 적어...  estp         0   \n","\n","   label_SN  label_TF  label_JP  \n","0         0         0         0  \n","1         1         0         1  \n","2         1         1         1  \n","3         0         0         0  \n","4         0         0         1  \n"]}]},{"cell_type":"code","source":["mbti_counts = df['label'].value_counts()\n","\n","# mbti 별 data 갯수 체크\n","mbti_counts_df = mbti_counts.reset_index()\n","mbti_counts_df.columns = ['text', 'label']\n","\n","print(mbti_counts_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsZ7JuVKidIr","executionInfo":{"status":"ok","timestamp":1717641767640,"user_tz":-540,"elapsed":5,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"outputId":"3a787131-e2ad-48f9-f34a-d44e557cdce1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    text  label\n","0   infp   4140\n","1   entp   3006\n","2   intp   2982\n","3   intj   2863\n","4   infj   2639\n","5   istp   2613\n","6   enfp   2362\n","7   isfp   2333\n","8   enfj   1547\n","9   entj   1475\n","10  isfj   1332\n","11  estp   1272\n","12  estj   1194\n","13  istj   1104\n","14  esfj   1102\n","15  esfp   1062\n"]}]},{"cell_type":"code","source":["pip install wordcloud matplotlib pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3Bs4FBxQo9D","executionInfo":{"status":"ok","timestamp":1717655746468,"user_tz":-540,"elapsed":8192,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"outputId":"3620ad77-c601-4b53-902e-ddd63f8535f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wordcloud\n","  Downloading wordcloud-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.1/511.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.25.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (10.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Installing collected packages: wordcloud\n","Successfully installed wordcloud-1.9.3\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","# 데이터 불러오기\n","data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/mbti_dataset.csv')\n","\n","# MBTI 유형 목록\n","mbti_types = [\n","    'istj', 'isfj', 'infj', 'intj',\n","    'istp', 'isfp', 'infp', 'intp',\n","    'estp', 'esfp', 'enfp', 'entp',\n","    'estj', 'esfj', 'enfj', 'entj'\n","]\n","\n","# 워드 클라우드 생성 함수\n","def generate_wordcloud(text, title):\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.title(title)\n","    plt.axis('off')\n","    plt.show()\n","\n","# 각 MBTI 유형별로 워드 클라우드 생성\n","for mbti in mbti_types:\n","    # 해당 유형의 텍스트 데이터 추출\n","    mbti_data = data[data['label'] == mbti]\n","    text = ' '.join(mbti_data['text'].values)\n","\n","    # 워드 클라우드 생성 및 시각화\n","    generate_wordcloud(text, title=f'Word Cloud for {mbti.upper()}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"alZEmRt7HOQe","executionInfo":{"status":"error","timestamp":1717655132904,"user_tz":-540,"elapsed":522,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"outputId":"07bcec55-4536-493d-a891-cc826d47bdd6"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'wordcloud'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-5709236fe656>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 데이터 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["longest_string = df['text'].astype(str).map(len).max()\n","longest_string_row = df[df['text'].astype(str).map(len) == longest_string]\n","# longest_string_row\n","df.iloc[3285]['text']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"iSnhThsblKw7","executionInfo":{"status":"ok","timestamp":1717589482825,"user_tz":-540,"elapsed":363,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"outputId":"621f820f-d1fc-4a33-c829-e519f1dc19e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'했습니다.[SEP] 오늘 행복하게 보낼 수 있으려나[SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","# 텍스트와 새로운 라벨 열들로 데이터 준비\n","data = [{\n","    'text': row['text'],\n","    'label_EI': row['label_EI'],\n","    'label_SN': row['label_SN'],\n","    'label_TF': row['label_TF'],\n","    'label_JP': row['label_JP']\n","} for row in df[['text', 'label_EI', 'label_SN', 'label_TF', 'label_JP']].to_dict('records')]\n","\n","# 데이터셋을 train, val, test 세트로 분할\n","train_data, tmp_test_data = train_test_split(data, test_size=0.2, random_state=42)\n","val_data, test_data = train_test_split(tmp_test_data, test_size=0.5, random_state=42)\n","\n","# 데이터 크기 출력\n","print(f\"Train data size: {len(train_data)}\")\n","print(f\"Validation data size: {len(val_data)}\")\n","print(f\"Test data size: {len(test_data)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJbakoputSRo","executionInfo":{"status":"ok","timestamp":1717642353544,"user_tz":-540,"elapsed":3739,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"outputId":"2865f115-2dc7-4b9a-aaad-5dca32289805"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train data size: 26420\n","Validation data size: 3303\n","Test data size: 3303\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Tv2HbuiBnqz","executionInfo":{"status":"ok","timestamp":1717642381446,"user_tz":-540,"elapsed":25523,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"colab":{"base_uri":"https://localhost:8080/","height":276,"referenced_widgets":["9ac54dd3f5954f83afd0d202c3d3ebdd","907b57a3957048beb562dc4ecb287a03","296ba6c74d0a4785a5eb3004f4327a58","830ffddaa3a44bbb85587872f86862d9","5bd1a674a3a049398f8d3d7e4717f9ed","f3ffb4f469c14dacacadf2c4beda7564","182f329649bb403e9970efe6acb4df96","24406d7fe4b94db39242050897e18219","f8367a93e27342e8868bf7f75ed61184","b618525ab2f243599e8a0dfd5239597e","bcce9527f45149518d0247228d95ab45","3765334ba4b54c08932eabfeff5aff0b","64ca63bc7b8f4cd7ac656188f611e491","8cbc49f420b84e4d931ea1700959c634","7439d0f85bca49138d752840896d2bdb","a5f0fb0466c4429ba6d6d79a930ba64f","a0ba74240af84ffca5d1654abcdbf9df","846bd10bc65d407b8a58f99ecaaf168d","15793b67c32846cebe34b642a189f93f","7fbe5862517d411180b3236ae972b70e","8cc72a9a832d4f93accab7616d33740e","0167e7cf817f4070850327adfc33f3be","7cac051183ff4257a1afb6850a1cb38e","b7cd324cf9264a4abc233577c85873c0","9845541f0bd24fc1bb5c1fe70d772566","8c4e04febfc14b2d93413affbed86731","ba293e7d86154ce7bb7ed56ead992936","5e9f0ae521ea462ebf67173f37772740","3e66d3ec40c847e69ce117f8ad225b62","9a87dab0cd774acaae70c275985e14b3","9573626c2255449598355b9d009d8f2e","12b7ca8e0b59445583a2a65a092217ff","0939028159704bdd88769e92544f95a8"]},"outputId":"f28fe255-0f9a-464d-e4f2-8256d4b06d97"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ac54dd3f5954f83afd0d202c3d3ebdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/344k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3765334ba4b54c08932eabfeff5aff0b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cac051183ff4257a1afb6850a1cb38e"}},"metadata":{}}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel, AdamW\n","from tqdm import tqdm\n","\n","# 1. 데이터 전처리\n","tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n","\n","class MBTIDataset(Dataset):\n","    def __init__(self, data, max_len):\n","        self.data = data\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        text = self.data[index]['text']\n","        label_EI = self.data[index]['label_EI']\n","        label_SN = self.data[index]['label_SN']\n","        label_TF = self.data[index]['label_TF']\n","        label_JP = self.data[index]['label_JP']\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].squeeze(),\n","            'attention_mask': encoding['attention_mask'].squeeze(),\n","            'label_EI': torch.tensor(label_EI),\n","            'label_SN': torch.tensor(label_SN),\n","            'label_TF': torch.tensor(label_TF),\n","            'label_JP': torch.tensor(label_JP)\n","        }\n","\n","# 2. 데이터로더 준비\n","def clean_data(data):\n","    return [item for item in data if isinstance(item['text'], str) and item['text'].strip()]\n","\n","train_dataset = MBTIDataset(clean_data(train_data), max_len=128)\n","val_dataset = MBTIDataset(clean_data(val_data), max_len=128)\n","test_dataset = MBTIDataset(clean_data(test_data), max_len=128)\n","\n","batch_size = 384\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruALiWx1LaG8"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel, AdamW\n","from tqdm import tqdm\n","\n","# GPU 사용 가능 여부 확인\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")\n","\n","# 1. BERT 모델 및 분류기 준비\n","model = BertModel.from_pretrained(\"kykim/bert-kor-base\").to(device)\n","\n","# BERT 모델 파라미터를 고정 (freeze)\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# 분류기 정의\n","class MBTIMultiLabelClassifier(torch.nn.Module):\n","    def __init__(self, bert_model):\n","        super(MBTIMultiLabelClassifier, self).__init__()\n","        self.bert = bert_model\n","        self.classifier_EI = torch.nn.Linear(self.bert.config.hidden_size, 2)\n","        self.classifier_SN = torch.nn.Linear(self.bert.config.hidden_size, 2)\n","        self.classifier_TF = torch.nn.Linear(self.bert.config.hidden_size, 2)\n","        self.classifier_JP = torch.nn.Linear(self.bert.config.hidden_size, 2)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        logits_EI = self.classifier_EI(pooled_output)\n","        logits_SN = self.classifier_SN(pooled_output)\n","        logits_TF = self.classifier_TF(pooled_output)\n","        logits_JP = self.classifier_JP(pooled_output)\n","        return logits_EI, logits_SN, logits_TF, logits_JP\n","\n","classifier = MBTIMultiLabelClassifier(model).to(device)\n","\n","# 2. 손실 함수와 옵티마이저 정의\n","loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = AdamW(classifier.parameters(), lr=5e-3) # 0.005\n","\n","# 3. 학습 loop\n","epochs = 3  # DataSet을 1회 온전하게 Weight Update하는데 사용.\n","for epoch in range(epochs):\n","    model.eval()  # BERT 모델을 평가 모드로 설정 (학습 비활성화)\n","    classifier.train()  # 분류기는 학습 모드로 설정\n","    loop = tqdm(train_loader)\n","    for batch in loop:\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        label_EI = batch['label_EI'].to(device)\n","        label_SN = batch['label_SN'].to(device)\n","        label_TF = batch['label_TF'].to(device)\n","        label_JP = batch['label_JP'].to(device)\n","\n","        with torch.no_grad():  # BERT 모델의 기울기 계산 비활성화\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","\n","        logits_EI, logits_SN, logits_TF, logits_JP = classifier(input_ids, attention_mask)\n","        loss_EI = loss_fn(logits_EI, label_EI)\n","        loss_SN = loss_fn(logits_SN, label_SN)\n","        loss_TF = loss_fn(logits_TF, label_TF)\n","        loss_JP = loss_fn(logits_JP, label_JP)\n","        loss = loss_EI + loss_SN + loss_TF + loss_JP\n","        loss.backward()\n","        optimizer.step()\n","        loop.set_postfix(loss=loss.item())\n","\n","    classifier.eval()  # 분류기를 평가 모드로 설정\n","    val_loss = 0\n","    val_acc_EI = 0\n","    val_acc_SN = 0\n","    val_acc_TF = 0\n","    val_acc_JP = 0\n","    for batch in val_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        label_EI = batch['label_EI'].to(device)\n","        label_SN = batch['label_SN'].to(device)\n","        label_TF = batch['label_TF'].to(device)\n","        label_JP = batch['label_JP'].to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits_EI, logits_SN, logits_TF, logits_JP = classifier(input_ids, attention_mask)\n","\n","            loss_EI = loss_fn(logits_EI, label_EI)\n","            loss_SN = loss_fn(logits_SN, label_SN)\n","            loss_TF = loss_fn(logits_TF, label_TF)\n","            loss_JP = loss_fn(logits_JP, label_JP)\n","            loss = loss_EI + loss_SN + loss_TF + loss_JP\n","\n","            val_loss += loss.item()\n","            preds_EI = torch.argmax(logits_EI, dim=1)\n","            preds_SN = torch.argmax(logits_SN, dim=1)\n","            preds_TF = torch.argmax(logits_TF, dim=1)\n","            preds_JP = torch.argmax(logits_JP, dim=1)\n","            val_acc_EI += (preds_EI == label_EI).sum().item()\n","            val_acc_SN += (preds_SN == label_SN).sum().item()\n","            val_acc_TF += (preds_TF == label_TF).sum().item()\n","            val_acc_JP += (preds_JP == label_JP).sum().item()\n","\n","    val_loss /= len(val_loader)\n","    val_acc_EI /= len(val_dataset)\n","    val_acc_SN /= len(val_dataset)\n","    val_acc_TF /= len(val_dataset)\n","    val_acc_JP /= len(val_dataset)\n","    val_acc = (val_acc_EI + val_acc_SN + val_acc_TF + val_acc_JP) / 4\n","    print(f'Epoch {epoch+1} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n","    print(f'Label Val Acc - EI: {val_acc_EI:.4f}, SN: {val_acc_SN:.4f}, TF: {val_acc_TF:.4f}, JP: {val_acc_JP:.4f}')\n","\n","# 6. 모델 평가 및 저장\n","model.eval()\n","classifier.eval()\n","test_acc_EI = 0\n","test_acc_SN = 0\n","test_acc_TF = 0\n","test_acc_JP = 0\n","for batch in test_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    label_EI = batch['label_EI'].to(device)\n","    label_SN = batch['label_SN'].to(device)\n","    label_TF = batch['label_TF'].to(device)\n","    label_JP = batch['label_JP'].to(device)\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits_EI, logits_SN, logits_TF, logits_JP = classifier(input_ids, attention_mask)\n","        preds_EI = torch.argmax(logits_EI, dim=1)\n","        preds_SN = torch.argmax(logits_SN, dim=1)\n","        preds_TF = torch.argmax(logits_TF, dim=1)\n","        preds_JP = torch.argmax(logits_JP, dim=1)\n","        test_acc_EI += (preds_EI == label_EI).sum().item()\n","        test_acc_SN += (preds_SN == label_SN).sum().item()\n","        test_acc_TF += (preds_TF == label_TF).sum().item()\n","        test_acc_JP += (preds_JP == label_JP).sum().item()\n","test_acc_EI /= len(test_dataset)\n","test_acc_SN /= len(test_dataset)\n","test_acc_TF /= len(test_dataset)\n","test_acc_JP /= len(test_dataset)\n","test_acc = (test_acc_EI + test_acc_SN + test_acc_TF + test_acc_JP) / 4\n","print(f'Test Acc: {test_acc:.4f} | EI: {val_acc_EI:.4f} | SN: {test_acc_SN:.4f} | TF: {val_acc_TF:.4f} | JP: {val_acc_JP:.4f}')\n","\n","# 모델 저장\n","classifier.cpu()\n","classifier_weight_path = '/content/drive/MyDrive/Colab Notebooks/kobert-mbti-classifier-softmax.pt'\n","torch.save(classifier.state_dict(), classifier_weight_path)\n","\n","optimizer_state_path = '/content/drive/MyDrive/Colab Notebooks/kobert-mbti-optimizer-softmax.pt'\n","training_state_path = '/content/drive/MyDrive/Colab Notebooks/kobert-mbti-training-state-softmax.pt'\n","\n","training_state = {\n","    'epoch': epoch + 1,\n","    'model_state_dict': model.state_dict(),\n","    'classifier_state_dict': classifier.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict()\n","}\n","\n","torch.save(optimizer.state_dict(), optimizer_state_path)\n","torch.save(training_state, training_state_path)\n"]},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertModel\n","\n","# GPU 사용 여부\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# 토크나이저 및 모델 로드\n","classifier_weight_path = '/content/drive/MyDrive/Colab Notebooks/kobert-mbti-classifier-softmax.pt'\n","tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n","model = BertModel.from_pretrained('kykim/bert-kor-base').to(device)\n","\n","classifier = MBTIMultiLabelClassifier(model).to(device)\n","\n","# 가중치 로드\n","classifier.load_state_dict(torch.load(classifier_weight_path))\n","model.eval()\n","classifier.eval()\n","model.to(device)\n","\n","# 입력된 문장을 기반으로 MBTI 예측 함수\n","def predict_mbti(text):\n","    # 입력된 문장을 토큰화\n","    encoding = tokenizer.encode_plus(\n","        text,\n","        max_length=128,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='pt'\n","    )\n","    input_ids = encoding['input_ids'].to(device)\n","    attention_mask = encoding['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        # BERT 모델을 사용하여 특징 추출\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","\n","        # 분류기를 통해 각 MBTI 차원 예측\n","        logits_EI, logits_SN, logits_TF, logits_JP = classifier(input_ids, attention_mask)\n","\n","    # 각 차원에 대해 가장 높은 확률을 가진 클래스 선택\n","    pred_EI = torch.argmax(logits_EI, dim=1).item()\n","    pred_SN = torch.argmax(logits_SN, dim=1).item()\n","    pred_TF = torch.argmax(logits_TF, dim=1).item()\n","    pred_JP = torch.argmax(logits_JP, dim=1).item()\n","\n","    # 예측된 클래스를 MBTI 유형으로 변환\n","    mbti = ''\n","    mbti += 'E' if pred_EI == 0 else 'I'\n","    mbti += 'S' if pred_SN == 0 else 'N'\n","    mbti += 'T' if pred_TF == 0 else 'F'\n","    mbti += 'J' if pred_JP == 0 else 'P'\n","\n","    return mbti\n","\n","# 문장 입력받아 예측\n","input_text = \"현재 너무 화나서 아이스크림을 사러가고 있어\"\n","predicted_mbti = predict_mbti(input_text)\n","print(f\"Predicted MBTI: {predicted_mbti}\")\n","\n","torch.cuda.empty_cache()\n","if torch.cuda.is_available():\n","  torch.cuda.empty_cache()\n","  torch.cuda.ipc_collect()"],"metadata":{"id":"Ntx-whHdiqCx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717639730389,"user_tz":-540,"elapsed":2108,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"outputId":"0604d00b-305d-49a4-a3da-d06a94b4ddf0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted MBTI: ENFP\n"]}]},{"cell_type":"code","source":["## ablation study ##\n","# 1. BERT 모델의 특정 레이어 제거\n","# 2. 분류기 헤드 변경(최종)\n","# 3. 특정 입력 특징 제거 stop words?\n","# 4. 파라미터 민감도 분석(learning rate, batch size 조정해보기)\n","\n","# 1. 특정 레이어 제거\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, AdamW\n","from tqdm import tqdm\n","from transformers import BertModel\n","import torch.nn as nn\n","\n","# 데이터 전처리 및 데이터 로더 설정\n","tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n","\n","# 데이터 로더 준비\n","def clean_data(data):\n","    return [item for item in data if isinstance(item['text'], str) and item['text'].strip()]\n","\n","train_dataset = MBTIDataset(clean_data(train_data), max_len=128)\n","val_dataset = MBTIDataset(clean_data(val_data), max_len=128)\n","test_dataset = MBTIDataset(clean_data(test_data), max_len=128)\n","\n","batch_size = 384\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# GPU 사용 여부 확인\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")\n","\n","class ModifiedBertModel(BertModel):\n","    def __init__(self, config, ablation_layer):\n","        super().__init__(config)\n","        self.ablation_layer = ablation_layer\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n","        outputs = super().forward(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=True,  # 모든 레이어의 히든 상태를 출력하도록 설정\n","            return_dict=return_dict\n","        )\n","\n","        hidden_states = outputs.hidden_states  # 모든 레이어의 히든 상태\n","\n","        # 특정 레이어를 제거하고 나머지 레이어의 hidden state를 concat\n","        modified_hidden_states = torch.cat([hidden_state for i, hidden_state in enumerate(hidden_states) if i != self.ablation_layer], dim=-1)\n","\n","        return modified_hidden_states\n","\n","train_data = clean_data(train_data)\n","val_data = clean_data(val_data)\n","test_data = clean_data(test_data)\n","\n","# BERT 모델 및 분류기 준비\n","ABLATION_LAYER = 8  # 제거할 BERT 레이어\n","\n","model = ModifiedBertModel.from_pretrained(\"kykim/bert-kor-base\", ablation_layer=ABLATION_LAYER).to(device)\n","\n","# 분류기 정의\n","class MBTIMultiLabelClassifier(nn.Module):\n","    def __init__(self, bert_model):\n","        super(MBTIMultiLabelClassifier, self).__init__()\n","        self.bert = bert_model\n","        hidden_size = self.bert.config.hidden_size * (self.bert.config.num_hidden_layers)\n","        self.classifier_EI = nn.Linear(hidden_size, 2)\n","        self.classifier_SN = nn.Linear(hidden_size, 2)\n","        self.classifier_TF = nn.Linear(hidden_size, 2)\n","        self.classifier_JP = nn.Linear(hidden_size, 2)\n","\n","    def forward(self, input_ids, attention_mask):\n","        modified_hidden_states = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = modified_hidden_states.mean(dim=1)  # 평균 풀링(입력 시퀀스의 모든 토큰에 대한 히든 상태의 평균을 계산하여 하나의 벡터로 만듬)\n","        logits_EI = self.classifier_EI(pooled_output)\n","        logits_SN = self.classifier_SN(pooled_output)\n","        logits_TF = self.classifier_TF(pooled_output)\n","        logits_JP = self.classifier_JP(pooled_output)\n","        return logits_EI, logits_SN, logits_TF, logits_JP\n","\n","classifier = MBTIMultiLabelClassifier(model).to(device)\n","\n","# 손실 함수와 옵티마이저 정의\n","loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = AdamW(classifier.parameters(), lr=5e-3)\n","\n","# 학습 및 평가 함수\n","def train_and_evaluate(model, classifier, train_loader, val_loader, test_loader, epochs=3):\n","    loss_fn = nn.CrossEntropyLoss().to(device)\n","    optimizer = AdamW(classifier.parameters(), lr=5e-3)  # 0.005\n","\n","    for epoch in range(epochs):\n","        model.eval()  # BERT 모델을 평가 모드로 설정 (학습 비활성화)\n","        classifier.train()  # 분류기는 학습 모드로 설정\n","        loop = tqdm(train_loader)\n","        for batch in loop:\n","            optimizer.zero_grad()\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            label_EI = batch['label_EI'].to(device)\n","            label_SN = batch['label_SN'].to(device)\n","            label_TF = batch['label_TF'].to(device)\n","            label_JP = batch['label_JP'].to(device)\n","\n","            with torch.no_grad():  # BERT 모델의 기울기 계산 비활성화\n","                modified_hidden_states = model(input_ids, attention_mask=attention_mask)\n","\n","            pooled_output = modified_hidden_states.mean(dim=1)  # 평균 풀링\n","            logits_EI, logits_SN, logits_TF, logits_JP = classifier(input_ids, attention_mask)\n","            loss_EI = loss_fn(logits_EI, label_EI)\n","            loss_SN = loss_fn(logits_SN, label_SN)\n","            loss_TF = loss_fn(logits_TF, label_TF)\n","            loss_JP = loss_fn(logits_JP, label_JP)\n","            loss = loss_EI + loss_SN + loss_TF + loss_JP\n","            loss.backward()\n","            optimizer.step()\n","            loop.set_postfix(loss=loss.item())\n","\n","        classifier.eval()  # 분류기를 평가 모드로 설정\n","        val_loss = 0\n","        val_acc_EI = 0\n","        val_acc_SN = 0\n","        val_acc_TF = 0\n","        val_acc_JP = 0\n","        for batch in val_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            label_EI = batch['label_EI'].to(device)\n","            label_SN = batch['label_SN'].to(device)\n","            label_TF = batch['label_TF'].to(device)\n","            label_JP = batch['label_JP'].to(device)\n","\n","            with torch.no_grad():\n","                modified_hidden_states = model(input_ids, attention_mask=attention_mask)\n","                pooled_output = modified_hidden_states.mean(dim=1)\n","                logits_EI, logits_SN, logits_TF, logits_JP = classifier(input_ids, attention_mask)\n","\n","                loss_EI = loss_fn(logits_EI, label_EI)\n","                loss_SN = loss_fn(logits_SN, label_SN)\n","                loss_TF = loss_fn(logits_TF, label_TF)\n","                loss_JP = loss_fn(logits_JP, label_JP)\n","                loss = loss_EI + loss_SN + loss_TF + loss_JP\n","\n","                val_loss += loss.item()\n","                preds_EI = torch.argmax(logits_EI, dim=1)\n","                preds_SN = torch.argmax(logits_SN, dim=1)\n","                preds_TF = torch.argmax(logits_TF, dim=1)\n","                preds_JP = torch.argmax(logits_JP, dim=1)\n","                val_acc_EI += (preds_EI == label_EI).sum().item()\n","                val_acc_SN += (preds_SN == label_SN).sum().item()\n","                val_acc_TF += (preds_TF == label_TF).sum().item()\n","                val_acc_JP += (preds_JP == label_JP).sum().item()\n","\n","        val_loss /= len(val_loader)\n","        val_acc_EI /= len(val_dataset)\n","        val_acc_SN /= len(val_dataset)\n","        val_acc_TF /= len(val_dataset)\n","        val_acc_JP /= len(val_dataset)\n","        val_acc = (val_acc_EI + val_acc_SN + val_acc_TF + val_acc_JP) / 4\n","        print(f'Epoch {epoch+1} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n","        print(f'Label Val Acc - EI: {val_acc_EI:.4f}, SN: {val_acc_SN:.4f}, TF: {val_acc_TF:.4f}, JP: {val_acc_JP:.4f}')\n","\n","# 모델 학습 및 평가\n","num_epochs = 3\n","train_and_evaluate(model, classifier, train_loader, val_loader, test_loader, epochs=num_epochs)\n"],"metadata":{"id":"R2u0FDEYJ33M","executionInfo":{"status":"ok","timestamp":1717655130768,"user_tz":-540,"elapsed":4800038,"user":{"displayName":"여행이쟈","userId":"02836985489279042441"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"58488560-1787-4fbf-b81c-d9d60ce49a67"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Device: cpu\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 69/69 [55:20<00:00, 48.12s/it, loss=6.05]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1 | Val Loss: 5.6017 | Val Acc: 0.5325\n","Label Val Acc - EI: 0.6210, SN: 0.6152, TF: 0.5005, JP: 0.3936\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 69/69 [55:19<00:00, 48.11s/it, loss=3.7]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Val Loss: 2.9075 | Val Acc: 0.4723\n","Label Val Acc - EI: 0.6140, SN: 0.3585, TF: 0.5244, JP: 0.3924\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 69/69 [55:01<00:00, 47.85s/it, loss=8.78]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Val Loss: 6.5845 | Val Acc: 0.5911\n","Label Val Acc - EI: 0.6210, SN: 0.6415, TF: 0.4941, JP: 0.6076\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SmBaKBIZR15S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예측 함수\n","def predict(text, model, classifier, tokenizer, device):\n","    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","    input_ids = inputs[\"input_ids\"]\n","    attention_mask = inputs[\"attention_mask\"]\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        modified_hidden_states, pooled_output = outputs\n","        logits_EI, logits_SN, logits_TF, logits_JP = classifier(pooled_output)\n","        preds_EI = torch.argmax(logits_EI, dim=1).item()\n","        preds_SN = torch.argmax(logits_SN, dim=1).item()\n","        preds_TF = torch.argmax(logits_TF, dim=1).item()\n","        preds_JP = torch.argmax(logits_JP, dim=1).item()\n","\n","    mbti_type = \"\"\n","    if preds_EI == 0:\n","        mbti_type += \"E\"\n","    else:\n","        mbti_type += \"I\"\n","    if preds_SN == 0:\n","        mbti_type += \"S\"\n","    else:\n","        mbti_type += \"N\"\n","    if preds_TF == 0:\n","        mbti_type += \"T\"\n","    else:\n","        mbti_type += \"F\"\n","    if preds_JP == 0:\n","        mbti_type += \"J\"\n","    else:\n","        mbti_type += \"P\"\n","\n","    return mbti_type\n","\n","# 예측 코드 실행\n","text = \"저는 책 읽는 것을 좋아하고 혼자 있는 시간을 즐기는 편입니다.\"\n","mbti_type = predict(text, model, classifier, tokenizer, device)\n","print(f\"예측된 MBTI 유형: {mbti_type}\")"],"metadata":{"id":"NiDSmmM2RwOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","if torch.cuda.is_available():\n","  torch.cuda.empty_cache()\n","  torch.cuda.ipc_collect()"],"metadata":{"id":"8GaMJdrIWY1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"WamNxc8Ml2JT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YoYP_nODHpUa"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[{"file_id":"1vLPjEps1QDAVH93cUvRoh527K8ZrwxZk","timestamp":1717507778504}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9ac54dd3f5954f83afd0d202c3d3ebdd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_907b57a3957048beb562dc4ecb287a03","IPY_MODEL_296ba6c74d0a4785a5eb3004f4327a58","IPY_MODEL_830ffddaa3a44bbb85587872f86862d9"],"layout":"IPY_MODEL_5bd1a674a3a049398f8d3d7e4717f9ed"}},"907b57a3957048beb562dc4ecb287a03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3ffb4f469c14dacacadf2c4beda7564","placeholder":"​","style":"IPY_MODEL_182f329649bb403e9970efe6acb4df96","value":"tokenizer_config.json: 100%"}},"296ba6c74d0a4785a5eb3004f4327a58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24406d7fe4b94db39242050897e18219","max":80,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8367a93e27342e8868bf7f75ed61184","value":80}},"830ffddaa3a44bbb85587872f86862d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b618525ab2f243599e8a0dfd5239597e","placeholder":"​","style":"IPY_MODEL_bcce9527f45149518d0247228d95ab45","value":" 80.0/80.0 [00:00&lt;00:00, 6.51kB/s]"}},"5bd1a674a3a049398f8d3d7e4717f9ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3ffb4f469c14dacacadf2c4beda7564":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"182f329649bb403e9970efe6acb4df96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24406d7fe4b94db39242050897e18219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8367a93e27342e8868bf7f75ed61184":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b618525ab2f243599e8a0dfd5239597e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcce9527f45149518d0247228d95ab45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3765334ba4b54c08932eabfeff5aff0b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64ca63bc7b8f4cd7ac656188f611e491","IPY_MODEL_8cbc49f420b84e4d931ea1700959c634","IPY_MODEL_7439d0f85bca49138d752840896d2bdb"],"layout":"IPY_MODEL_a5f0fb0466c4429ba6d6d79a930ba64f"}},"64ca63bc7b8f4cd7ac656188f611e491":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ba74240af84ffca5d1654abcdbf9df","placeholder":"​","style":"IPY_MODEL_846bd10bc65d407b8a58f99ecaaf168d","value":"vocab.txt: 100%"}},"8cbc49f420b84e4d931ea1700959c634":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15793b67c32846cebe34b642a189f93f","max":344259,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7fbe5862517d411180b3236ae972b70e","value":344259}},"7439d0f85bca49138d752840896d2bdb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cc72a9a832d4f93accab7616d33740e","placeholder":"​","style":"IPY_MODEL_0167e7cf817f4070850327adfc33f3be","value":" 344k/344k [00:00&lt;00:00, 6.24MB/s]"}},"a5f0fb0466c4429ba6d6d79a930ba64f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ba74240af84ffca5d1654abcdbf9df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"846bd10bc65d407b8a58f99ecaaf168d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15793b67c32846cebe34b642a189f93f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fbe5862517d411180b3236ae972b70e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8cc72a9a832d4f93accab7616d33740e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0167e7cf817f4070850327adfc33f3be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7cac051183ff4257a1afb6850a1cb38e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7cd324cf9264a4abc233577c85873c0","IPY_MODEL_9845541f0bd24fc1bb5c1fe70d772566","IPY_MODEL_8c4e04febfc14b2d93413affbed86731"],"layout":"IPY_MODEL_ba293e7d86154ce7bb7ed56ead992936"}},"b7cd324cf9264a4abc233577c85873c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e9f0ae521ea462ebf67173f37772740","placeholder":"​","style":"IPY_MODEL_3e66d3ec40c847e69ce117f8ad225b62","value":"config.json: 100%"}},"9845541f0bd24fc1bb5c1fe70d772566":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a87dab0cd774acaae70c275985e14b3","max":725,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9573626c2255449598355b9d009d8f2e","value":725}},"8c4e04febfc14b2d93413affbed86731":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12b7ca8e0b59445583a2a65a092217ff","placeholder":"​","style":"IPY_MODEL_0939028159704bdd88769e92544f95a8","value":" 725/725 [00:00&lt;00:00, 83.4kB/s]"}},"ba293e7d86154ce7bb7ed56ead992936":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e9f0ae521ea462ebf67173f37772740":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e66d3ec40c847e69ce117f8ad225b62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a87dab0cd774acaae70c275985e14b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9573626c2255449598355b9d009d8f2e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12b7ca8e0b59445583a2a65a092217ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0939028159704bdd88769e92544f95a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}